# Server
AI_SERVICE_HOST=0.0.0.0
AI_SERVICE_PORT=8000
LOG_LEVEL=info
# Enable uvicorn reload in dev runner
AI_SERVICE_RELOAD=0
# Optional: explicit uv executable for the dev runner
UV_EXE=

# Device routing (preferred)
# Allowed values: CPU|GPU|NPU or openvino:CPU|openvino:GPU|openvino:NPU
DEVICES_CLIP=GPU
DEVICES_BLIP=GPU
DEVICES_LLM=NPU
# LLM can run on GPU as well (set DEVICES_LLM=GPU)
DEVICES_STRICT=true

# Legacy single-device override (only used if DEVICES_* are not set)
AI_DEVICE=

# OpenVINO cache (persistent compile cache)
OV_CACHE_DIR=models/.ov_cache

# Safety: disallow CPU fallback
ENABLE_CPU_FALLBACK=0

# Model storage (local/offline)
MODEL_DIR=models
MODEL_CACHE_DIR=models
OFFLINE=1
MODEL_FETCH_MODE=never

# Model paths
OV_CLIP_DIR=models/clip
OV_CAPTION_DIR=models/caption
OV_LLM_DIR=models/llm

# Model sources
CLIP_SOURCE=hf_export
CAPTION_HF_ID=Salesforce/blip-image-captioning-base
LLM_SOURCE=prebuilt_ov_ir
LLM_HF_OV_REPO=llmware/qwen2.5-3b-instruct-ov
LLM_HF_ID=Qwen/Qwen2.5-3B-Instruct
LLM_REVISION=

# Pipeline limits
MAX_TAGS=10
MAX_CAPTIONS_PER_IMAGE=1

# LLM generation
LLM_MAX_NEW_TOKENS=220
LLM_TEMPERATURE=0.4
# Robust Qwen-style stop markers (note: "### User" alone is not enough; models often keep going without turn markers).
LLM_STOP_STRINGS="You are an AI assistant|Yes_or_No|### User|### System|### Assistant"

# Timeouts (seconds)
REQUEST_TIMEOUT_SEC=30
LLM_TIMEOUT_SECONDS=20

# Debugging
DEBUG=0
DEBUG_AI=0
DEBUG_AI_INCLUDE_PROMPT=0
DEBUG_AI_MAX_TAGS_LOG=50
DEBUG_AI_MAX_CHARS=6000
DEBUG_AI_RESPONSE=1
